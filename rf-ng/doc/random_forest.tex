\documentclass{article}
\usepackage{lucidabr,amsmath}
\usepackage{verbatim,fancyvrb}
\usepackage{color,mycode}
\usepackage{graphicx}
\usepackage{adhoc,url}
\usepackage{hyperref}

\usepackage[letterpaper,body={6.3in,9.15in},top=.8in,left=1.1in]{geometry}
% \usepackage[a4paper,body={6.1in,9.7in},top=.8in,left=1.1in]{geometry}

\hypersetup{pdftitle={random_forest 0.4},
            pdfauthor={Allin Cottrell},
            colorlinks=true,
            linkcolor=blue,
            urlcolor=red,
            citecolor=steel,
            bookmarks=true,
            bookmarksnumbered=true,
            plainpages=false
}

\begin{document}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1ex}

\title{random\_forest 0.4} \author{Allin Cottrell}
\maketitle

This is an unusual gretl package---unusual in that it relies on the
\textsf{R} package \textsf{randomForest} for its underlying
functionality,\footnote{See
  \url{https://cran.r-project.org/web/packages/randomForest/index.html}.}
and therefore also relies on \textsf{R} itself. A native
implementation of the Random Forest machine-learning algorithm may be
added to gretl at some point, but for now the aim of this package is
more modest: simply to offer a gretl-style scripting interface, as
well as a gretl-style GUI interface, to functionality borrowed from
the \textsf{R} ecosystem.

This package was last verified as working with \textsf{R} version
4.3.0 (2023-04-21) and \textsf{randomForest} 4.7-1.1 (2022-05-23),
which requires \textsf{R} 4.1.0 or higher. Some details about ensuring
the \textsf{R} setup that's required can be found in
Section~\ref{sec:Rdetails}.

I'm not going to try to explain here what the Random Forest algorithm
is all about---Wikipedia does a good job,\footnote{See
  \url{https://en.wikipedia.org/wiki/Random_forest}.} and you can
consult Leo Breiman's original paper on the topic.\footnote{See
  \url{https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf}.}
I'll just say that it's a predictive apparatus somewhat comparable
with a Support Vector Machine or SVM, something that is supported
natively in gretl---see ``gretl + SVM'' under the \textsf{Help} menu
in the gretl GUI.

\section{Scripting usage}

The function available for command-line and scripting use is named
\texttt{random\_forest}. It has the following signature:
%
\begin{code}
bundle random_forest (const series y, list X, const bundle opts)
\end{code}
%
This means that it returns a bundle and has three required arguments,
a series, a list and a bundle. The first two arguments specify a
dependent variable and list of independent variables (or ``features''
as they're known in the machine-learning world). The \texttt{opts}
bundle has just one required member, namely a specification of the
observations to be used for training. (The remaining observations, of
which there must be some, will be used for testing.) This can take
either of two forms:
\begin{itemize}
\item \texttt{n\_train}: an integer. The first \texttt{n\_train} valid
  observations will be used for training. Or
\item \texttt{d\_train}: a 0/1 dummy series. Observations for which
  \texttt{d\_train} has value 1 will be used for training.
\end{itemize}
Exactly one of these specifications should be given, but if you supply
both then \texttt{n\_train} takes precedence and \texttt{d\_train} is
ignored.

What you get on return is a bundle containing predictions for both the
training and the testing data, along with some figures of merit with
which to assess the degree of predictive success.  We'll expand on the
content of this bundle shortly, but first here's some detail on the
options. Besides \texttt{n\_train} or \texttt{d\_train}, the
\texttt{opts} bundle may contain any of the following:

\begin{center}
\begin{tabular}{llll}
  \textit{key} & \textit{type} & \textit{comment} & \textit{default value} \\[4pt]
  \texttt{verbose} & integer & controls printed output & 1 \\
  \texttt{classify} & boolean & classification (vs. regression) & automatic\\
  \texttt{tune} & boolean & tune the \texttt{mtry} parameter & 1 or TRUE \\
  \texttt{seed} & integer & seed for RNG in \textsf{R} & automatic
\end{tabular}
\end{center}

The default \texttt{verbose} value of 1 prints summary output compiled
by gretl. The other acceptable values are 0, for no printed output, or
2, to include a trace of the progress made by \textsf{R}'s
\textsf{randomForest}.

\subsection{Classification vs Regression}

The Random Forest algorithm can handle both ``classification'' (for
discrete, qualitative outcomes) and ``regression'' (for a more or less
continuous numerical outcome). By default gretl calls for
classification if and only if the dependent variable is a 0/1 dummy
variable or is marked as an ``encoding'' (see the help for the
\texttt{setinfo} command).  But you can force the issue by setting
\texttt{classify} to 0 or 1. You may wish to take control when the
dependent is ordinal, but not on an interval or ratio scale.

\subsection{Tuning}

You'll probably want to let the underlying engine do some tuning,
which involves a call to the \texttt{tuneRF} function in the
\textsf{R} package. The tunable parameter \texttt{mtry} represents the
number of covariates randomly sampled as candidates at each split in
tree formation; \texttt{tuneRF} optimizes this parameter with respect
to an ``Out-of-Bag'' error estimate.  However, if you prefer to use
the default value of \texttt{mtry}, give \texttt{0} or \texttt{FALSE}
for the \texttt{tune} option. In classification mode the default is
$\sqrt{k}$ and in regression mode it's $k/3$, where $k$ is the number
of covariates.

\subsection{Control of randomization}

As the name implies, there's randomization involved in building a
Random Forest. If you want to get perfectly reproducible results you
should set \texttt{seed} to a specific value in the options bundle;
otherwise the results will differ to some degree across runs of a
given script.

\subsection{Illustration}

We'll illustrate a call to \texttt{random\_forest} by reference to the
sample script included in the package.  It begins with
%
\begin{code}
include random_forest.gfn
open titanic3.gdt -q --frompkg=random_forest
set seed 877777
series sorter = uniform()
dataset sortby sorter
\end{code}
%
The data file is a slightly modified version of \texttt{titanic3.csv}
from \url{https://hbiostat.org/data/}; it contains 1306 records
regarding passengers on RMS \textit{Titanic}.\footnote{This is a
  superset of the \textit{Titanic} data available on
  \texttt{kaggle.com}.}  The target for prediction is the binary
variable \texttt{survived} (1 = survived, 0 = didn't). Seven other
variables are available as candidate predictors.  The last two lines
of code above randomize the order of the observations prior to
division into training and testing samples.  (You can comment out the
\texttt{set seed} statement to get a different randomization on each
invocation of the script).  The code then continues:
%
\begin{code}
list X = pclass female sibsp parch fare embarked
bundle opts = _(n_train=900, seed=777778)
bundle rfb = random_forest(survived, X, opts)
\end{code}
%
So we're using six of the covariates, selecting the first 900 records
for training, setting a random seed for replicability (this seed will
be used by \textsf{R}), and implicitly accepting the default values
for \texttt{classify}, \texttt{tune} and \texttt{verbose}.

Two of the available covariates in the \texttt{titanic3} dataset call
for some discussion. First, \texttt{embarked} is an encoding of a
purely qualitative characteristic: the port at which the passenger
embarked. Variables of this sort are automatically converted into a
set of 0/1 dummies ($m-1$ of them, if the variable has $m$ distinct
values) before being passed to \texttt{R}. Second, \texttt{age} (the
age of the passenger in years) may be a plausible predictor of
survival but unfortunately this datum is missing in 263 cases so we
omitted it from the specification. However, one \textit{could} include
\texttt{age}: gretl would then automatically shrink the sample to skip
the missing values.

Partial output from the sample script is shown in
Listing~\ref{output}.

\begin{script}[htbp]
  \begin{scodebit}
Called randomForest() in classification mode
Dependent variable: survived
Total observations 1306: 900 training, 406 testing
RNG seed = 777778

Training percent correct: 78.33
Testing percent correct:  80.30

Confusion matrix, training:
             0            1

0          473           71
1          124          232

Confusion matrix, testing:
             0            1

0          226           38
1           42          100
\end{scodebit}
  \caption{Output from sample script}
  \label{output}
\end{script}

\subsection{The returned bundle}

The bundle returned by \texttt{random\_forest} in classification mode
contains the following items.
\begin{itemize}
\item \texttt{insample} and \texttt{outsample}; column vectors holding
  the predictions for the training and testing observations,
  respectively.
\item \texttt{allpred}: for convenience, a series holding all the
  predictions.
\item \texttt{n1} and \texttt{n2}: the numbers of training and testing
  observations.
\item \texttt{seed}: the RNG seed, if specified.
\item \texttt{ntree}: the number of trees created.
\item \texttt{mtry}: the mtry value employed.
\item \texttt{importance}: a column vector holding an ``importance''
  value for each feature.
\item \texttt{pcc1} and \texttt{pcc2}: scalars giving the percentage
  of correct predictions for the training and testing data
  respectively.
\item \texttt{C1} and \texttt{C2}: confusion matrices pertaining to
  the training and testing data.
\end{itemize}

In case the mode is regression rather than classification,
\texttt{pcc1} and \texttt{pcc2} are replaced by \texttt{MSE1} and
\texttt{MSE2}, giving the mean squared error for the two sub-samples.
Also \texttt{C1} and \texttt{C2} are replaced by a single matrix named
\texttt{regstats}, an example of which is shown below.
%
\begin{code}
         Training      Testing 
  ME       453.91      -8666.1 
RMSE       44776.       51341. 
 MAE       29757.       36842. 
 MPE      -5.2256      -13.368 
MAPE       15.756       22.104
\end{code}
%
The abbreviated row labels in \texttt{regstats} expand as follows:
\texttt{ME} = mean error, \texttt{RMSE} = root mean squared error,
\texttt{MAE} = mean absolute error, \texttt{MPE} = mean percentage
error, and \texttt{MAPE} = mean absolute percentage error.  The
``percentage'' measures may be missing if the actual values include
exact zeros.

\subsection{Other scriptable functions}

Besides the primary function, \texttt{random\_forest}, two additional
functions are available in scripting mode, namely \texttt{rf\_print}
and \texttt{rf\_plot}. The signature of \texttt{rf\_print} is:
%
\begin{code}
void rf_print (const bundle *b, int minimal[0:1:0])
\end{code}
For the first argument, a pointer to a bundle returned by
\texttt{random\_forest} is expected. Nothing is returned, but the
content of the bundle is printed in a standard form. The second
argument, which defaults to 0, is only relevant for GUI usage:
its effect is to suppress output which would have already been shown
via the GUI.

The signature of \texttt{rf\_plot} is:
%
\begin{code}
void rf_plot (const bundle *b, int maxvars[2::15], string outspec[null])
\end{code}
%
This function produces a plot (a horizontal bar-chart) showing the
relative importance of the ``features'' in the prediction of
outcomes. As with \texttt{rf\_print}, the first argument should be a
pointer to a bundle returned by \texttt{random\_forest}. The second
argument, \texttt{maxvars}, limits the number of features to show, the
default value being 15. You can give an integer argument less than or
greater than 15 to tighten or relax this limit. The third argument,
\texttt{outspec}, can be used to set the destination of the plot. By
default it's shown on screen but you can give a filename to direct the
output to file. See the help for the \texttt{gnuplot} command for the
mapping from filename to file type.

\section{GUI usage}

On installing the package you should be given the option of adding a
hook named \textsf{Random Forest} to gretl's \textsf{Model} menu,
under the \textsf{Robust estimation} sub-menu. This hook brings up a
dialog box in which you can specify most of inputs described above
(Figure~\ref{fig:dialog}). Clicking \textsf{OK} in this dialog will
open a window displaying the results of estimation and allowing you to
save the results bundle or any of its contents.

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.6]{rfgui.png}
  \caption{The random\_forest dialog}
  \label{fig:dialog}
\end{figure}

\section{Change log}

2024-03-21: version 0.4: update ui-maker syntax, require gretl 2024a.\\
2023-12-18: version 0.3: fix \texttt{rf\_plot} for the case where at
least one feature has a name that starts with \texttt{e} or \texttt{E}.\\
2023-12-16: version 0.2: require R shared library, and require gretl
2023c to ensure that \texttt{R.dll} can be found on MS Windows; update GUI.\\
2023-03-09: version 0.1 released.

\section{Some \textsf{R} details}
\label{sec:Rdetails}

As noted above, this gretl package requires version 4.7-1.1 of the
\textsf{randomForest} package, which in turn requires \textsf{R} 4.1.0
or higher. As of this writing it is verified as working with
\textsf{R} 4.2.1, 4.2.2 and 4.3.0. It is straightforward to install
\textsf{R} for most platforms: see
\url{https://cran.r-project.org/}. Having done so one can then install
\textsf{randomForest} using the \textsf{R} command:
\begin{code}
install.packages("randomForest")
\end{code}
This should take care of installing the other \textsf{R} packages on
which \textsf{randomForest} depends if they're not already present on
the local machine. But please note that with this release we also
require that the \textsf{R} shared library is installed; this should
usually be the case unless you build \textsf{R} yourself and choose
not to build the shared library.

We mentioned above that gretl by default calls \textsf{randomForest}'s
\texttt{tuneRF} function to optimize its \texttt{mtry}
parameter. We're aware that the \textsf{caret} package for \textsf{R}
offers various means of tuning machine-learning algorithms, including
Random Forests, but chose not to build in support for \textsf{caret}
in this offering. That's because our (admittedly limited) testing led
to the conclusion that \texttt{tuneRF} is about equally effective but
also because \textsf{caret} has a very hefty list of dependencies, and
its complexity seems inconsistent with our wish to offer a relatively
quick and easy route to the exploration of Random Forests. That said,
it's possible that a future version of this package might add some
degree of \textsf{caret} support if a demand is evident.

\end{document}
